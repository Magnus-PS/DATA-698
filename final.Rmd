---
title: 'DATA 698 Final Research Project'
author: "Magnus Skonberg"
date: "`r Sys.Date()`" # Due 5/11/2021
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: no
    theme: flatly
    highlight: tango
  pdf_document:
    latex_engine: xelatex
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

</style>
```

```{r setup, include=FALSE}
#Import libraries
library(tidyverse)
library(dplyr)
library(readr)
library(ggplot2)
library(RCurl)
library(rvest)
library(stringr)
library(tidyr)
library(kableExtra)
library(BBmisc)
library(tm)
library(sqldf)
library(inspectdf)
library(corrplot)
library(MASS)
library(caret)
library(glmnet)

options(scipen = 9)
set.seed(123)

#---User-defined function(s)---#

#Adapted correlation matrix used in EDA section:
plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor, use = 'na.or.complete')
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}

#Evaluation metrics used in Model Building / Selection section:
eval_metrics = function(model, df, predictions, target){
    resids = df[,target] - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = as.character(round(summary(model)$r.squared, 4))
    adj_r2 = as.character(round(summary(model)$adj.r.squared, 4))
    print(adj_r2) #Adjusted R-squared
    print(as.character(round(sqrt(sum(resids2)/N), 4))) #RMSE
}

```

## Abstract

TBD

## Introduction

### Author

### The Problem

## Literature Review

To be amended ...

## Methodology

* Data Gathering & Pre-Processing
* Data Exploration & Preparation
* Multivariate Regression Analysis


## Data Gathering & Preprocessing

### Dependent Variable Creation

In order to create our dependent 'health score' variable, I first familiarized myself with the data at hand. 

Life expectancy, obesity, and physical activity data were downloaded from the [Institute for Health Metrics and Evaluation](http://www.healthdata.org/us-health/data-download) and converted to a csv-compatible form, where then a subset of columns were selected for our consideration for the creation of a dependent 'health score' variable.

* For **life expectancy data** we read in: `Male life expectancy, 2010 (years)`, `Female life expectancy, 2010 (years)`, `Difference in male life expectancy, 1985-2010 (years)`, and	`Difference in female life expectancy, 1985-2010 (years)`.
* For **obesity data** we read in: `Male obesity prevalence, 2009 (%)`, `Female obesity  prevalence, 2009 (%)`, `Difference in male obesity prevalence, 2001-2009 (percentage points)`, and `Difference in female obesity prevalence, 2001-2009 (percentage points)`.
* For **physical activity data** we read in: `Male sufficient physical activity  prevalence, 2009 (%)`, `Female sufficient physical activity  prevalence, 2009 (%)`, `Difference in male sufficient physical activity prevalence, 2001-2009 (percentage points)`, and `Difference in female sufficient physical activity prevalence, 2001-2009 (percentage points)`.

For sake of conciseness, the majority of exploratory details and plots for our dependent variable's creation have been remitted (although the code is available in the Appendix). Prior to moving on though, it is worth noting a few points that were observed prior to the normalization and congregation of our sub-variables:

* With regard to **longevity**: on average, males live to be ~75 years old while females live to be ~80 years old. Thus, females live ~5yrs more than males on average. On average, male life expectancy increased by ~4 years while female life expectancy increased by ~1.5 years. Thus, male life expectancy increased at a greater rate than female life expectancy from 1985-2010.
* With regard to **obesity**: on average, 38% of females were obese whereas 36% of males were. Thus, females have a *slightly* higher incidence of obesity than males. On average, the male obesity rate increased by ~7.2% while the rate of female obesity increased by ~6.7%. Thus, males got fatter at a greater rate than females from 2001 to 2009.
* With regard to **physical activity**: on average, 55% of males vs. 48.7% of females received sufficient physical activity in 2009. Thus, males reported a higher level of physical activity. On average, males had a 1.9% increase in physical activity from 2001 to 2009 whereas females reported a 4.7% increase over the same period. Thus, females increased their activity levels at a greater rate than males from 2001 to 2009.

Our dependent variable is built upon these sub-variables. The above notes provide context regarding the variable upon which our regression model is to be fastened.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}

## --- Dependent Variable Creation --- ##

#Read in life expectancy data, convert to tibble, and select pertinent columns:
longevity <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/IHME_LifeExpectancy.csv")
life_table <- as_tibble(longevity)
life_table <- life_table %>% dplyr::select(1:2,13:16) %>% na.omit()

#Read in obesity data, convert to tibble, and select pertinent columns:
obesity <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/IHME_Obesity.csv")
obesity_table <- as_tibble(obesity)
obesity_table <- obesity_table %>% dplyr::select(1:2,5:6,9:10) %>% na.omit()

#Read in physical activity data, convert to tibble, and select pertinent columns:
activity <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/IHME_PhysicalActivity.csv")
act_table <- as_tibble(activity)
act_table <- act_table %>% dplyr::select(1:2,5:6,9:10) %>% na.omit()

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}
###LIFE EXPECTANCY DATA: exploration, normalization, and compilation

#Explore life expectancy data at a county level:
glimpse(life_table)
summary(life_table)

#Extract variables of interest
m1 <- life_table$`Male life expectancy, 2010 (years)`
f1 <- life_table$`Female life expectancy, 2010 (years)`
dm1 <- life_table$`Difference in male life expectancy, 1985-2010 (years)`
df1 <- life_table$`Difference in female life expectancy, 1985-2010 (years)`

#Normalize data scale to be from 0 to 1
n_m1 = (m1-min(m1))/(max(m1)-min(m1))
n_f1 = (f1-min(f1))/(max(f1)-min(f1))
n_dm1 = (dm1-min(dm1))/(max(dm1)-min(dm1))
n_df1 = (df1-min(df1))/(max(df1)-min(df1))

#Histogram of original vs. normalized data
##Life expectancy histograms
par(mfrow=c(2,2))
hist(m1, breaks=10, xlab="Age (years)", col="lightblue", main="Male life expectancy, 2010")
hist(n_m1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Male life expectancy, 2010")
hist(f1, breaks=10, xlab="Age (years)", col="lightblue", main="Female life expectancy, 2010")
hist(n_f1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Female life expectancy, 2010")

##Longevity improvement histograms
par(mfrow=c(2,2))
hist(dm1, breaks=10, xlab="Age (years)", col="lightblue", main="Male longevity improvement, 1985-2010")
hist(n_dm1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Male longevity improvement, 1985-2010")
hist(df1, breaks=10, xlab="Age (years)", col="lightblue", main="Female longevity improvement, 1985-2010")
hist(n_df1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Female longevity improvement, 1985-2010")

#Add normalized variables together
life <- n_m1 + n_dm1 + n_f1 + n_df1

#Normalize activity to 0-1 range
n_life = (life-min(life))/(max(life)-min(life))
#head(n_life)

#Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(life, breaks=10, xlab="Score", col="lightblue", main="Longevity metric")
hist(n_life, breaks=10, xlab="Normalized Score", col="lightblue", main="Longevity metric")
summary(n_life) #slight left skew

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}
###OBESITY DATA: exploration, normalization, and compilation

#Explore obesity data at a county level:
glimpse(obesity_table)
summary(obesity_table)

#Extract variables of interest
m2 <- obesity_table$`Male obesity prevalence, 2009 (%)` 
f2 <- obesity_table$`Female obesity  prevalence, 2009 (%)` 
dm2 <- obesity_table$`Difference in male obesity prevalence, 2001-2009 (percentage points)` 
df2 <- obesity_table$`Difference in female obesity prevalence, 2001-2009 (percentage points)`

#Normalize
n_m2 = (m2-min(m2))/(max(m2)-min(m2))
n_f2 = (f2-min(f2))/(max(f2)-min(f2))
n_dm2 = (dm2-min(dm2))/(max(dm2)-min(dm2))
n_df2 = (df2-min(df2))/(max(df2)-min(df2))

#Histogram of original vs. normalized data
par(mfrow=c(2,2))
hist(m2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Male obesity prevalence, 2009")
hist(n_m2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Male obesity prevalence, 2009")
hist(f2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Female obesity prevalence, 2009")
hist(n_f2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female obesity prevalence, 2009")

par(mfrow=c(2,2))
hist(dm2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Male obesity increase, 2001-2009")
hist(n_dm2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Male obesity increase, 2001-2009")
hist(df2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Female obesity increase, 2001-2009")
hist(n_df2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female obesity increase, 2001-2009")

#Add normalized variables together
fat <- n_m2 + n_dm2 + n_f2 + n_df2

#Normalize activity to 0-1 range
n_fat = (fat-min(fat))/(max(fat)-min(fat))

#head(n_fat)

# Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(fat, breaks=10, xlab="Score", col="lightblue", main="Obesity metric")
hist(n_fat, breaks=10, xlab="Normalized Score", col="lightblue", main="Obesity metric")
summary(n_fat) #right skewed

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}
###PHYSICAL ACTIVITY DATA: exploration, normalization, and compilation

glimpse(act_table)
summary(act_table)

#Explore and normalize male physical activity data
m3 <- act_table$`Male sufficient physical activity  prevalence, 2009 (%)`
f3 <- act_table$`Female sufficient physical activity  prevalence, 2009 (%)`
dm3 <- act_table$`Difference in male sufficient physical activity prevalence, 2001-2009 (percentage points)`  
df3 <- act_table$`Difference in female sufficient physical activity prevalence, 2001-2009 (percentage points)` 

#Normalized Data
n_m3 = (m3-min(m3))/(max(m3)-min(m3))
n_f3 = (f3-min(f3))/(max(f3)-min(f3))
n_dm3 = (dm3-min(dm3))/(max(dm3)-min(dm3))
n_df3 = (df3-min(df3))/(max(df3)-min(df3))

#Histogram of original vs. normalized data
par(mfrow=c(2,2))
hist(m3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Male activity prevalence, 2009")
hist(n_m3, breaks=10, xlab="Normalized physical activity rate (%)", col="lightblue", main="Male activity prevalence, 2009")
hist(f3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Female activity prevalence, 2009")
hist(n_f3, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female activity prevalence, 2009")

par(mfrow=c(2,2))
hist(dm3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Male activity difference, 2001-2009")
hist(n_dm3, breaks=10, xlab="Normalized physical activity rate (%)", col="lightblue", main="Male activity difference, 2001-2009")
hist(df3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Female activity difference, 2009")
hist(n_df3, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female activity difference, 2009")

#Add normalized variables together
active <- n_m3 + n_dm3 + n_f3 + n_df3

#Normalize activity to 0-1 range
n_active = (active-min(active))/(max(active)-min(active))

#head(n_active)

# Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(active, breaks=10, xlab="Score", col="lightblue", main="Physical activity metric")
hist(n_active, breaks=10, xlab="Normalized Score", col="lightblue", main="Physical activity metric")

summary(n_active) #slight right skew

```

For each over-arching variable (life expectancy, obesity, or physical activity), we read in the 4 corresponding variables listed above, normalize, compile the over-arching variable as a summation of the normalized sub-variables and then normalize the result. 

The normalization of sub-variables and (later) over-arching variables was done in order to bring all data to a 0-to-1 scale via the following equation:

$$
Transformed.Values = \frac{(Values - Min)}{(Max - Min)}
$$

Upon normalization of our over-arching variables, we created our dependent 'healthy lifestyle' variable as a combination of longevity, obesity, and physical activity:

$$
Lifestyle = Normalized.Life - Normalized.Obesity + Normalized.Activity
$$

The result was normalized, bringing our cycle of thrice normalizing and twice congregating to a close, with a dependent variable on a 0 to 1 scale:

```{r, eval = TRUE, echo = FALSE}

###DEPENDENT VARIABLE CREATION: sum, normalize, and plot

#Add normalized variables together
lifestyle <- n_life - n_fat + n_active

#Normalize health to 0-1 range
normalized_lifestyle = (lifestyle-min(lifestyle))/(max(lifestyle)-min(lifestyle))

#Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(lifestyle, breaks=10, xlab="Score", col="lightblue", main="Health metric")
hist(normalized_lifestyle, breaks=10, xlab="Normalized Score", col="lightblue", main="Health metric")

summary(normalized_lifestyle)
#head(normalized_lifestyle)

```

For our 'healthy lifestyle' metric, we observe a normal distribution whose peak is centered between 0.4 and 0.5. When we consult the summary statistics, we verify a mean of 0.4639 and a median of 0.4642. Confirming that our dependent variable is slightly left skewed.

### Top 10 Healthiest Counties

As a next step, we utilize our health score metric to filter through county data for the top 10 healthiest counties:

```{r, eval = TRUE, echo = FALSE}
#create new df with state | county | health score
starter_df <- life_table %>% 
    dplyr::select(1:2)

starter_df$health_score <- normalized_lifestyle
healthiest_counties <- filter(starter_df, `health_score` > 0.895) #top 10
healthiest_counties <- healthiest_counties[order(-healthiest_counties$`health_score`),] #descending order

#head(starter_df)
#nrow(healthiest_counties) #10
healthiest_counties %>%
  kbl() %>%
  kable_minimal()

```


From the above list, we note (6) Colorado, (2) California, (1) Utah and (1) Wyoming county. From this, we extend a few assumptions regarding factors that might come into play for the healthiest counties: 

* sunshine, 
* median income,
* sparser population clusters (aside from San Fransisco), and
* friendliness to an active, healthy lifestyle.

We note these factors with interest and plan to revisit our assumptions later. We can observe the variables that are included in our optimal multi regression model as well as those that appear to carry the most predictive impact (ie. largest coefficients).

With our health metric created and a brief investigation into the top 10 healthiest counties, we move on to reading in, exploring, and preparing our independent variables.


### Independent Variable Preprocessing

We sought county-level data where the counties of the United States would form the basis of our observations and variables could include numerous standard and non-standard health-related metrics:

* `Alcohol Consumption` sourced from [IHME](http://ghdx.healthdata.org/record/ihme-data/united-states-alcohol-use-prevalence-county-2002-2012).
* `Heart Disease` sourced from the [IHME](http://ghdx.healthdata.org/record/ihme-data/united-states-cardiovascular-disease-mortality-rates-county-1980-2014)
* `Education`, `Unemployment`, and `Population` sourced from the [USDA](https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/).
* `Environmental Quality Index` sourced from the [EPA](https://edg.epa.gov/EPADataCommons/public/ORD/CPHEA/EQI_2006_2010/).
* `Food Insecurity` sourced **by request** from [Feeding America](https://www.feedingamerica.org/research/map-the-meal-gap/by-county).
* `Sunlight` sourced from [CDC Wonder](https://wonder.cdc.gov/controller/datarequest/D80;jsessionid=097497410B08E44FD89ECC2AB08F?stage=results&action=sort&direction=MEASURE_DESCEND&measure=D80.M1).
* `Poverty` and `MedianIncome` sourced from the [US Census Bureau](https://www.census.gov/data/datasets/2019/demo/saipe/2019-state-and-county.html).

While the IHME is an independent global health research center associated with the University of Washington and Feeding America is one of the largest nonprofit organizations in the United States, every other data source is connected to the United States government. **The reputability and dependability of the source / institution motivated the election of these sources. **

Datasets were downloaded from their respective platforms, simplified to a .csv-compatible form (obviously impertinent variables and header rows were dropped), uploaded to Github and then read in via R's built-in read_csv() function. This operation was completed (9) times because the majority of our variables were listed in separate sets.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}

## --- Independent Variable Preprocessing --- ##

#Read in alcohol data, convert to tibble, and drop impertinent observation:
alcohol <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/AlcoholConsumption.csv")
alcohol <- as_tibble(alcohol)
alcohol <- alcohol[-1,] #drop State = National
#dim(alcohol) #3178 x 6

alcohol <- alcohol[alcohol$Location != alcohol$State,] #drop state observations from Location column
#dim(alcohol) #3178 - 3127 = 51 observations dropped

#rename columns
alcohol <- alcohol %>% rename( 
    County = Location,
    Hvy = Hvy_2012,
    Bng = Binge_2012,
    HvyPctChg = `HvyPctChg_2005-2012`,
    BngPctChg = `BingePctChg_2005-2012`)

#drop excess verbage from County column
stopwords <- c("and", "Area", "Borough", "Census", "City", "County", "Division", "Municipality", "Parish")
alcohol$County <- removeWords(alcohol$County, stopwords)
#head(alcohol)

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in heart data, convert to tibble, and drop impertinent observation:
heart <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/CardiovascularDisease.csv")
heart <- as_tibble(heart)
heart <- subset(heart, select = -c(`Mortality Rate, 2010*`)) #drop 2010
heart <- heart[-1,] #drop State = National
#dim(heart) #3193 x 4

# remove State == Location
heart <- heart[heart$Location != heart$State,]
dim(heart) #3193 - 3143 = 50 observations removed

# retitle columns
heart <- heart %>% rename( 
    County = Location,
    Mortality_2005 = `Mortality Rate, 2005*`,
    Mortality_2014 = `Mortality Rate, 2014*`)

# retain value EXCLUSIVELY for Mortality Rate columns
heart$Mortality_2005 <- gsub("\\s*\\([^\\)]+\\)","",as.character(heart$Mortality_2005))
heart$Mortality_2014 <- gsub("\\s*\\([^\\)]+\\)","",as.character(heart$Mortality_2014))

#convert columns to proper type
heart$Mortality_2005 <- as.double(heart$Mortality_2005)
heart$Mortality_2014 <- as.double(heart$Mortality_2014)

#drop excess verbage from County column
heart$County <- removeWords(heart$County, stopwords)
heart$County <- gsub("(.*),.*", "\\1", heart$County) #remove everything after comma

# add Chg column
heart$MortalityChg <- heart$Mortality_2014 - heart$Mortality_2005

#finalize format of df
heart <- subset(heart, select = -c(`Mortality_2005`)) #drop 2005
heart <- heart %>% rename( Mortality = Mortality_2014)

#head(heart)

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in education data, convert to tibble, and drop impertinent observation:
education <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Education.csv")
education <- as_tibble(education)
education <- education[-1,] #drop State = National

education$State <- state.name[match(education$State, state.abb)] #convert state abbreviation to name

education <- education[education$`Area name` != education$State,] #drop state observations from Area name column
#dim(education) #3281 - 3233 = 48 observations dropped

#rename columns
education <- education %>% rename(
    County = `Area name`,
    LTHighSchool = `Percent of adults with less than a high school diploma, 2015-19`,
    HighSchool = `Percent of adults with a high school diploma only, 2015-19`,
    SomeCollege = `Percent of adults completing some college or associate's degree, 2015-19`,
    College = `Percent of adults with a bachelor's degree or higher, 2015-19`)

#drop excess verbage from County column
education$County <- removeWords(education$County, stopwords)

#head(education) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in eqi data and convert to tibble:
eqi <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/EnvironmentalQualityIndex.csv")
eqi <- as_tibble(eqi)
eqi <- subset(eqi, select = -c(3:7)) #drop indices that makeup EQI score

eqi$State <- state.name[match(eqi$State, state.abb)] #convert state abbreviation to name

#rename columns
eqi <- eqi %>% rename(
    County = County_Name,
    EQI = environmental_quality_index)

#drop excess verbage from County column
eqi$County <- removeWords(eqi$County, stopwords)

#head(eqi) #verify
#dim(eqi) #3281 x 6

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in food insecurity data and convert to tibble:
food <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/FoodInsecurity.csv")
food <- as_tibble(food)
#head(food)

#drop FIPS
food <- subset(food, select=-c(FIPS))
#dim(food) #3142 x 3: no need to drop observations

#convert State to full name
food$State <- state.name[match(food$State, state.abb)] #convert state abbreviation to name

#rename columns
food <- food %>% rename(
    County = `County, State`,
    FoodInsecurity = `2018 Food Insecurity Rate`)

#remove excess verbage from County
food$County <- removeWords(food$County, stopwords)
food$County <- gsub("(.*),.*", "\\1", food$County) #remove everything after comma

#drop % from Food Insecurity and convert to double
food$FoodInsecurity = as.double(gsub("[\\%,]", "", food$FoodInsecurity))

#head(food) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in sun data and convert to tibble:
sun <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Sunlight.csv")
sun <- as_tibble(sun)
#dim(sun) #3161 x 3

#rename column
sun <- sun %>% rename(Sun = `Avg Daily Sunlight`)

#drop excess verbage from County column
sun$County <- removeWords(sun$County, stopwords)
sun$County <- gsub("(.*),.*", "\\1", sun$County) #remove everything after comma

#head(sun) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in une ployment data and convert to tibble:
unemp <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Unemployment.csv")
unemp <- as_tibble(unemp)
unemp <- unemp[-1,] #drop State = National
unemp <- subset(unemp, select=-c(3)) #drop 2016

unemp$State <- state.name[match(unemp$State, state.abb)] #convert state abbreviation to name
unemp <- unemp[unemp$area_name != unemp$State,] #drop state observations from Area name column

unemp <- unemp %>% rename(
    County = area_name,
    Unemployment = Unemployment_rate_2019,
    UnemploymentChg = `Unemployment_chg_2016-2019`) #rename columns

unemp$County <- removeWords(unemp$County, stopwords) #drop excess verbage from County column
unemp$County <- gsub("(.*),.*", "\\1", unemp$County) #remove everything after comma

#dim(unemp) #3274 - 3224 = 50 dropped
#head(unemp) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in wealth data and convert to tibble:
wealth <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Wealth.csv")
wealth <- as_tibble(wealth)
wealth <- wealth[-1,] #drop State = National
#dim(wealth) #3193 x 4

wealth$State <- state.name[match(wealth$State, state.abb)] #convert state abbreviation to name
wealth <- wealth[wealth$County != wealth$State,] #drop state observations from Area name column
#dim(wealth) #3193 - 3143 = 50 observations dropped

#convert columns to proper type
wealth$PovertyRate <- as.double(wealth$PovertyRate)
wealth$MedianHouseholdIncome <- as.numeric(gsub(",","",wealth$MedianHouseholdIncome))

#rename columns
wealth <- wealth %>% rename(
    Poverty = PovertyRate,
    Income = MedianHouseholdIncome) #rename columns

wealth$County <- removeWords(wealth$County, stopwords) #drop excess verbage from County column

#head(wealth)

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in population data and convert to tibble:
pop <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/population.csv")
pop <- as_tibble(pop)

# rename columns
pop <- pop %>% rename(
    State = STNAME,
    County = CTYNAME,
    Pop_2010 = CENSUS2010POP,
    Population = POPESTIMATE2019,
    Births = BIRTHS2019,
    Deaths = DEATHS2019,
    NetMig = NETMIG2019) #rename columns

#add population change variable
pop$PopChg <- pop$Population - pop$Pop_2010

pop <- subset(pop, select=-c(3)) #drop 2010

#dim(pop) #3193 x 7
pop <- pop[pop$County != pop$State,] #drop state observations from Area name column
#dim(pop) #3193 - 3141 = 52 observations dropped

pop$County[1802] <- "Dona Ana County" #invalid UTF-8
pop$County <- removeWords(pop$County, stopwords) #drop excess verbage from County column

#head(pop)

```

Bringing each of our sources to a consistent format was of the utmost importance for us to merge dataframes and (later) explore our data.

In order to do so:

* impertinent variables and observations were dropped,
* where `State` names were abbreviated they were converted to the full name,
* excess verbage was dropped from our `County` variable, 
* percent change variables were added (where applicable),
* excess characters were dropped from observations (ie. "%" from percent change variables),
* variables were converted to the proper type (ie. numeric variable's listed as character type),
* the general consistency of variable names, type, and format was established, and
* all dataframes were merged based on matching `State` and `County` observations.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Merge df's

#1. merge health score and alcohol df's
##Trim white space
alcohol$County <- trimws(alcohol$County)
starter_df$County <- trimws(starter_df$County)
##SQL join
df <- sqldf("SELECT *
             FROM starter_df
             LEFT JOIN alcohol ON starter_df.State = alcohol.State AND starter_df.County = alcohol.County")
##remove extra State, County columns
df <- subset(df, select=-c(4,5))

#2. merge heart to df
##Trim white space
heart$County <- trimws(heart$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN heart ON df.State = heart.State AND df.County = heart.County")
##remove extra State, County columns
df <- subset(df, select=-c(8,9))

#3. merge education to df
##Trim white space
education$County <- trimws(education$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN education ON df.State = education.State AND df.County = education.County")
##remove extra State, County columns
df <- subset(df, select=-c(10,11))

#4. merge eqi to df
##Trim white space
eqi$County <- trimws(eqi$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN eqi ON df.State = eqi.State AND df.County = eqi.County")
##remove extra State, County columns
df <- subset(df, select=-c(14,15))

#5. merge food to df
##Trim white space
food$County <- trimws(food$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN food ON df.State = food.State AND df.County = food.County")
##remove extra State, County columns
df <- subset(df, select=-c(15,16))

#6. merge sun to df
##Trim white space
sun$County <- trimws(sun$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN sun ON df.State = sun.State AND df.County = sun.County")
##remove extra State, County columns
df <- subset(df, select=-c(16,17))

#7. merge unemp to df
##Trim white space
unemp$County <- trimws(unemp$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN unemp ON df.State = unemp.State AND df.County = unemp.County")
##remove extra State, County columns
df <- subset(df, select=-c(17,18))

#8. merge wealth to df
##Trim white space
wealth$County <- trimws(wealth$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN wealth ON df.State = wealth.State AND df.County = wealth.County")
##remove extra State, County columns
df <- subset(df, select=-c(19,20))

#9. merge pop to df
##Trim white space
pop$County <- trimws(pop$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN pop ON df.State = pop.State AND df.County = pop.County")
##remove extra State, County columns
df <- subset(df, select=-c(21,22))

#verify variables and dimensions
head(df) 
dim(df) #3154 x 25

```

The result of our merging all dataframes / variables into one master is a 3154 observation x 25 variable dataframe `df`. We'll explore the resulting data frame in the next section to see what insight we might glean to better inform our data preparation and model-building.


## Data Exploration & Preparation

The goal of exploratory data analysis (or EDA for short) is to really grasp and understand the data at hand.

For our approach, we first get to know the structure and value ranges, we then look at the distributions of our features, visualize the relationship our variables have with one another and the target variable via correlation matrix, and visit the proportion of missing data vs. correlation with target. After this point, we should have enough insight to prepare our data and then build our model.

To start, we utilize the built-in glimpse() and summary() methods to gain insight into the dimensions, variable characteristics, and value range for our training dataset:

```{r, echo = F, eval = T}

#baseline EDA: glimpse() and summary()
glimpse(df)
#summary(df)

```

We're dealing with 3154 observations x 25 variables: 2 categorical variables, 23 numeric variables, significant NA counts for numerous variables, and quite a difference in the magnitude of values based on the variables.

Our **categorical variables** are `State`, our state identifier and `County`, our county identifier. These variables are not of much use for anything beyond identification and may be excluded.

Our **dependent variable** `health_score` is quantitative and provides a measure of 'healthy lifestyle' as a function of longevity, obesity, and physical activity. This will be our "target" variable, the one against which we'll measure the impact of our independent variables.

As for our **quantitative, independent variables**:

* `Hvy`: % of population that drink heavily. *"Heavy" drinking is defined as the consumption, on average, of more than one drink per day for women or two drinks per day for men in the past 30 days.*
* `HvyPctChg`: % change in heavy drinking from 2005 to 2009.
* `Bng`: % of population that binge drink. *"Binge" drinking is defined as the consumption of more than four drinks for women or five drinks for men on a single occasion at least once in the past 30 days.*
* `BngPctChg`: % change in binge drinking from 2005 to 2009.
* `Mortality`: mortality rate in 2014 as a result of heart disease.
* `MortalityChg`: % change in mortality rate from 2005 to 2014 as a result of heart disease.
* `LTHighSchool`: % of population educated less than high school from 2015-2019.
* `HighSchool`: % of population educated at a high school level (and no further) from 2015-2019.
* `SomeCollege`: % of population who received some college education from 2015-2019.
* `College`: % of population who completed at least a Bachelor's level education from 2015-2019.
* `EQI`: Environmental Quality Index.
* `FoodInsecurity`: % of population whose food intake was disrupted by lack of resources in 2018.
* `Sun`: average daily sunlight (KJ/m^2) in 2011.
* `Unemployment`: rate of unemployment in 2019.
* `UnemploymentChg`: change in unemployment rate from 2015 to 2019.
* `Poverty`: rate of poverty in 2019.
* `Income`: median household income in 2019.
* `Population`: county-level population for 2019.
* `Births`: county-level births for 2019.
* `Deaths`: county-level deaths for 2019.
* `NetMig`: county-level net migration for 2019.
* `PopChg`: county-level rate of population change from 2010 to 2019.

As a next step, we drop NA values from consideration (150 observations) and turn our attention to exploring the relationship these variables have with one another and with the target via correlation matrix. We consider only variables with a correlation significance > 0.2 in our plot:

```{r, eval = T, echo = F}

#drop NAs from consideration
df <- drop_na(df)
#dim(df) #3154 - 3004 = 150 dropped observations

```

```{r, echo = F, eval = T, fig.height = 8, fig.width = 10}

#Select numeric variables
df_num <- as.data.frame(df[3:25])
#dim(df_num)
#head(df_num)

#Utilize custom-built correlation matrix generation function
plot_corr_matrix(df_num, 0.2)

```

The above plot is "busy", and from that, we may extend that multicollinearity is a concern. We may eliminate a large portion of the variables we've carried upto this point and proceed with only those with strong predictive promise and / or unique value added (ie. `BngPctChg` or `Sun`).

Based on these guidelines, it appears that we should proceed with at least `BngPctChg`, `Mortality`, `College`, `FoodInsecurity`, `Unemployment`, `Income`, and `PopChg`. These variables are relatively unique from one another and have a strong correlation with `health_score`.

To clarify we consulted a table (code available in Appendix) with the proportion of missing data and correlation with our target variable. We found that none of our independent variables are missing data, that `Hvy`, `MortalityChg`, `NetMig` have a weak correlation with `health_score`, and that the remainder of our variables carry a relatively strong correlation with the target.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}

#Compute proportion of missing data per variable
v <- colnames(df_num)
incomplete <- function(x) sum(!complete.cases(x)) / 3004
Missing_Data <- sapply(df_num[v], incomplete) 
#head(Missing_Data) #verify

#Compute correlation between each variable and TARGET
target_corr <- function(x, y) cor(y, x, use = "na.or.complete")
HealthScore_Corr <- sapply(df_num[v], target_corr, y=df_num$health_score) 
#head(HealthScore_Corr) #verify

#Bind and output Missing Data and Correlation with Target
MDHSC <- data.frame(cbind(Missing_Data, HealthScore_Corr))
MDHSC %>%
  kbl(caption = "Proportion of Missing Data vs. Correlation with Health Score") %>%
  kable_minimal()

```

To address weak target correlation and multicollinearity we consider the exclusion of 13 variables : `Hvy`, `MortalityChg`, and `NetMig` due to weak correlation with the target variable and `HvyPctChg` `Bng`, `LTHighSchool`, `HighSchool`, `SomeCollege`, `Unemployment`, `Poverty`, `Population`, `Births`, and `Deaths` due to multicollinearity.

Before doing so, we utilize the **Boruta** function for feature ranking and selection as confirmation (included in Appendix). In doing so, `MortalityChg`, `BngPctChg`, `Unemployment`, `HvyPctChg`, and `Population` were identified as the variables with the lowest importance. 

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE, fig.height = 8, fig.width = 10}
#Utilize Boruta for feature ranking and selection
library(Boruta)

# Perform Boruta search
boruta_output <- Boruta(health_score ~ ., data=na.omit(df_num), doTrace=0, maxRuns = 1000)

#Get significant variables including tentatives
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
#print(boruta_signif)

# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")

```

Additionally, we observed that `Mortality`, `College`, `FoodInsecurity`, `LTHighSchool`, `Sun`, `EQI`, `Income`, `PopChg`, `Bng`, and `UnemploymentChg` appear to be our strongest predictors (with the lowest incidence of multicollinearity).

Based on variable importance, correlation with `health_score` and multicollinearity, we proceed with these variables and visit their corresponding histograms:

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Feature selection (address weak target correlation and multicollinearity)
select_df <- df_num[, c(1,4,6,8,11:14,16,18,23)]
#head(select_df) #verify

```

```{r, eval = T, echo = F, message = F, warning = F, fig.height = 8, fig.width = 10}
#Histograms for all variables
select_df %>%
    gather() %>%
    ggplot(aes(value)) +
        facet_wrap(~ key, scales = "free", ncol=4) +
        geom_histogram(bins=20,color="darkblue", fill="lightblue")

```

From the histograms above, we note a number of **non-normal** (ie. `PopChg`), **bimodal** (ie. `Sun`), **normal right skewed** (ie. `College`), **normal left skewed** (ie. `EQI`), and relatively **normal** (ie. `Mortality`) distributions. 

The wide-ranging difference in scales and non-normal distributions may be problematic in applying generalized linear regression models. Normalization may address the scales issue while transformation, outlier removal, or feature creation (ie. converting non-normal variables to dummy "flag" variables) may address the issue of non-normal distributions.

................................................................................

First, we generate a baseline model with our 10 independent variables and observe an **R^2 of 0.6603**.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Ensure reproducibility
set.seed(123)

#Train-test split data
dt = sort(sample(nrow(select_df), nrow(select_df)*.75))
train <- select_df[dt,]
test <- select_df[-dt,] 
#dim(train) #2253 x 11
#dim(test) # 751 x 11

#Baseline model (model 1): prior to outlier handling, normalization, feature engineering
model_1 <- lm(health_score ~., data = train)
summary(model_1) #R^2 = 0.6603, vars = 10

```

We then set out to optimize `model_1` and the results of each step are summarized below:

* **Handling NA's (Imputation)**: 0 missing values. Inessential.
* **Normalization**: no positive impact on R^2 value. Commented out.
* **Outlier Handling**: used Cook's distance and found no positive impact on R^2 value no matter how much we varied our filter value. 
* **Feature Creation**: created 7 features and **improved R^2 to 0.6735**.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}

#Imputation / removal (address NAs): 0 missing values

#Normalization: 0 impact on R^2
norm_minmax <- function(x){(x- min(x, na.rm = TRUE)) /(max(x, na.rm = TRUE)-min(x, na.rm = TRUE))}
norm_df <- as.data.frame(lapply(train, norm_minmax))
#head(norm_df) #verify

#Outlier handling: reduced R^2
#cooksD <- cooks.distance(model_1)
#influential <- as.numeric(names(cooksD)[(cooksD > (10 * mean(cooksD, na.rm = TRUE)))])
#train[influential,] #verify outliers - 20 rows
#out_df <- train[-influential,]

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Feature engineering

#combine datasets so we don't have to make features twice
train$dataset <- 'train'
test$dataset <- 'test'
final_df <- rbind(train, test)

#Creating new features (started with 1st, 3rd quartile values then adjusted)
##strong EQI and lots of sun
final_df$env_and_sun <- as.factor(ifelse(final_df$EQI > 0.90017 & final_df$Sun > 16350, 1, 0))
##high proportion college graduates and high income (3rd quartile)
final_df$college_and_income <- as.factor(ifelse(final_df$College > 20.0 & final_df$Income > 65000, 1, 0))
##low binge drinking and high income (older, more responsible?)
final_df$lowDrink_and_HiIncome <- as.factor(ifelse(final_df$Bng < 32.5 & final_df$Income > 61629, 1, 0))
##high binge drinking and high income (expendable income, socially active)
final_df$HiDrink_and_HiIncome <- as.factor(ifelse(final_df$Bng > 35.0 & final_df$Income > 61629, 1, 0))
##high food insecurity and high mortality and less than HS
final_df$LowFood_HiDeath_LowEd <- as.factor(ifelse(final_df$FoodInsecurity > 15.7 & final_df$Mortality > 500.00 & final_df$LTHighSchool > 20.0, 1, 0))
##low income and less than HS and growth in unemployment
final_df$LoIncome_LowEd_HiUnemp <- as.factor(ifelse(final_df$Income < 30000 & final_df$UnemploymentChg < -1.7 & final_df$LTHighSchool > 16.8, 1, 0))
##big PopChg and low unemployment
final_df$HiPop_LoUnemp <- as.factor(ifelse(final_df$PopChg > 1595.5 & final_df$UnemploymentChg > -1, 1, 0))

#Transformed (model 2): after handling outliers, normalization, feature engineering
train2 <- final_df %>% filter(dataset == 'train') %>% dplyr::select(-dataset)
test2 <- final_df %>% filter(dataset == 'test') %>% dplyr::select(-dataset)
model_2 <- lm(health_score ~., data = train2)
#summary(model_2) #R^2 = 0.6735, vars = 17

```

As a final transformation step, we optimize on our AIC score - a measure of goodness of fit *and* model complexity - via StepAIC() function.

```{r, comment=FALSE, warning=FALSE, message=FALSE}
#AIC optimize the transformed model
#stepAIC(model_2)

model_3 <- lm(formula = health_score ~ Mortality + College + FoodInsecurity + 
    LTHighSchool + Sun + EQI + Income + PopChg + Bng + UnemploymentChg + 
    env_and_sun + lowDrink_and_HiIncome + HiDrink_and_HiIncome + 
    LowFood_HiDeath_LowEd + LoIncome_LowEd_HiUnemp + HiPop_LoUnemp, 
    data = train2)

#summary(model_3) #R^2 = 0.6734, vars = 16

# Predict and evaluate raw_lm model on training data
predictions = predict(model_3, newdata = train2)
eval_metrics(model_3, train2, predictions, target = 'health_score') #0.671, 0.0764

# Predict and evaluate raw_lm model on testing data
predictions = predict(model_3, newdata = test2)
eval_metrics(model_3, test2, predictions, target = 'health_score') #0.671, 0.0815

```

We then output resulting model metrics in tabular form for interpretation:

```{r, eval = T, echo = F, message = F, warning = F, fig.height = 8, fig.width = 10}
#Create Kable table to succinctly summarize model optimization results
# Model <- c('model_1', 'model_2', 'model_3')
# Var_Num <- c(10, 17, 16)
# R_Squared <- c(0.6603, 0.6735, 0.6734)
# RSE <- c(0.07808, 0.07667, 0.07667)
# 
# output <- cbind(Model, Var_Num, R_Squared, RSE)
# 
# output %>%
#   kbl(caption = "Results of Data Preparation") %>%
#   kable_minimal()

#REUSE THIS PORTION LATER FOR THE FINAL, COMPLETE TABLE (with all models)

```

Based on the three metrics noted above, we elect the AIC optimized model (`model_3`) to represent our linear model. It strikes the greatest balance between model complexity, goodness of fit and error.


## Multivariate Regression Analysis

In addition to the models we've already explored, we explore a linear regression model of our raw data, apply the stepAIC() function to observe the impact on model predictive metrics, and then proceed regularization (ridge and lasso) regression models on both the transformed and the raw data.

We then output the performance metrics of all models in a table to select the model with the strongest predictive promise. We evaluate performance using the R-squared value and Root Mean Squared Error (RMSE). Lower RMSE and higher R-squared values are indicative of a stronger model.

```{r}
#Linear regression model, raw data

#Train-test split data
dt2 = sort(sample(nrow(df_num), nrow(df_num)*.75))
train_raw <- df_num[dt2,]
test_raw <- df_num[-dt2,] 
#dim(train_raw) #2253 x 23
#dim(test_raw) # 751 x 23

#Train raw_lm model:
raw_lm <- lm(health_score ~., data = train_raw)
#summary(raw_lm) #R^2 = 0.6808, vars = 22

# Predict and evaluate raw_lm model on training data
predictions = predict(raw_lm, newdata = train_raw)
eval_metrics(raw_lm, train_raw, predictions, target = 'health_score') #0.6678, 0.0759

# Predict and evaluate raw_lm model on testing data
predictions = predict(raw_lm, newdata = test_raw)
eval_metrics(raw_lm, test_raw, predictions, target = 'health_score') #0.6678, 0.0827

```

```{r}
#AIC optimize linear regression model, raw data
#stepAIC(raw_lm)

#Train aic_raw_lm model:
aic_raw_lm <- lm(formula = health_score ~ Hvy + HvyPctChg + Bng + Mortality + 
    HighSchool + SomeCollege + College + EQI + FoodInsecurity + 
    Sun + Unemployment + UnemploymentChg + Poverty + Population + 
    Births + Deaths + NetMig, data = train_raw)

#summary(aic_raw_lm) #R^2 = 0.6802, vars = 17

# Predict and evaluate aic_raw_lm model on training data
predictions = predict(aic_raw_lm, newdata = train_raw)
eval_metrics(aic_raw_lm, train_raw, predictions, target = 'health_score') #0.6671, 0.0761

# Predict and evaluate aic_raw_lm model on testing data
predictions = predict(aic_raw_lm, newdata = test_raw)
eval_metrics(aic_raw_lm, test_raw, predictions, target = 'health_score') #0.6671, 0.0828

```


### Ridge Regression

We use the glmnet() package to build our regularized regression models (first ridge regression, then lasso regression). The glmnet function does not work with dataframes, so we make use of numeric matrices.

We use the dummyVars function from the caret package to create our model matrix. We then use the predict() function to create numeric model matrices from our training and test data.

```{r}
#Ridge regression, raw data

#Specify column names
cr_raw = c('Hvy', 'HvyPctChg', 'BngPctChg', 'MortalityChg', 'HighSchool', 'SomeCollege', 'College', 'Unemployment', 'Poverty', 'Population', 'Births', 'Deaths', 'NetMig', 'Mortality', 'FoodInsecurity', 'LTHighSchool', 'Sun', 'EQI', 'Income', 'PopChg', 'Bng', 'UnemploymentChg', 'health_score')

#Generate dummy variables from data (if applicable)
dummies <- dummyVars(health_score ~ ., data = df_num[,cr_raw])
train_dummies = predict(dummies, newdata = train_raw[,cr_raw]) #2253 x 22
test_dummies = predict(dummies, newdata = test_raw[,cr_raw]) #751 x 22
print(dim(train_dummies)); print(dim(test_dummies))

#Create numeric model matrices
x = as.matrix(train_dummies)
y_train = train_raw$health_score
x_test = as.matrix(test_dummies)
y_test = test_raw$health_score

lambdas <- 10^seq(2, -3, by = -.1) #specify lambda sequence

#Train model
raw_ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)
summary(raw_ridge_reg)

#Compute optimal lambda
raw_cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
ol <- cv_ridge$lambda.min
ol #0.003162278

#Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- round((1 - SSE / SST),4)
  RMSE = round(sqrt(SSE/nrow(df)),4)
  
  # Model performance metrics
data.frame(RMSE = RMSE, Rsquare = R_square)
}

#Predict and evaluate raw_ridge_reg model on training data
predictions_train <- predict(raw_ridge_reg, s = ol, newx = x)
eval_results(y_train, predictions_train, train_raw) #RMSE: 0.0763, RSquare: 0.6671

#Predict and evaluate raw_ridge_reg model on testing data
predictions_test <- predict(raw_ridge_reg, s = ol, newx = x_test)
eval_results(y_test, predictions_test, test_raw) #RMSE: 0.0791, RSquare: 0.6718
```

```{r}
#Ridge regression, transformed data

#Specify column names
cols_reg = c('Mortality', 'College', 'FoodInsecurity', 'LTHighSchool', 'Sun', 'EQI', 'Income', 'PopChg','Bng','UnemploymentChg','env_and_sun','lowDrink_and_HiIncome','HiDrink_and_HiIncome','LowFood_HiDeath_LowEd','LoIncome_LowEd_HiUnemp','HiPop_LoUnemp','health_score')

#Generate dummy variables from data (if applicable)
dummies <- dummyVars(health_score ~ ., data = dat[,cols_reg])
train_dummies2 = predict(dummies, newdata = train2[,cols_reg]) #2253 x 22
test_dummies2 = predict(dummies, newdata = test2[,cols_reg]) #751 x 22
print(dim(train_dummies2)); print(dim(test_dummies2))

#Create numeric model matrices
x2 = as.matrix(train_dummies2)
y_train2 = train2$health_score
x_test2 = as.matrix(test_dummies2)
y_test2 = test2$health_score

#Train model
ridge_reg = glmnet(x2, y_train2, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)
summary(ridge_reg)

#Compute optimal lambda
cv_ridge2 <- cv.glmnet(x2, y_train2, alpha = 0, lambda = lambdas)
ol2 <- cv_ridge2$lambda.min
ol2 #0.003162278

#Predict and evaluate ridge_reg model on training data
predictions_train2 <- predict(ridge_reg, s = ol2, newx = x2)
eval_results(y_train2, predictions_train2, train2) #RMSE: 0.0764, RSquare: 0.6733

#Predict and evaluate ridge_reg model on training data
predictions_test2 <- predict(ridge_reg, s = ol2, newx = x_test2)
eval_results(y_test2, predictions_test2, test2) #RMSE: 0.0815, RSquare: 0.6319

```


### Lasso Regression

Introduction

```{r}
#Lasso regression, raw data

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

#Compute optimal lambda
lambda_best <- lasso_reg$lambda.min 
lambda_best #0.001

raw_lasso <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(raw_lasso, s = lambda_best, newx = x)
eval_results(y_train, predictions_train, train_raw) #RMSE: 0.0767, Rsquare: 0.664

predictions_test <- predict(raw_lasso, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test, test_raw) #RMSE: 0.0792, Rsquare: 0.6712

```

```{r}
#Lasso regression, transformed data

# Setting alpha = 1 implements lasso regression
lasso_reg2 <- cv.glmnet(x2, y_train2, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

#Compute optimal lambda
lambda_best2 <- lasso_reg2$lambda.min 
lambda_best2 #0.001

lasso_model <- glmnet(x2, y_train2, alpha = 1, lambda = lambda_best2, standardize = TRUE)

predictions_train2 <- predict(lasso_model, s = lambda_best2, newx = x2)
eval_results(y_train2, predictions_train2, train2) #RMSE: 0.0765, Rsquare: 0.6724

predictions_test2 <- predict(lasso_model, s = lambda_best2, newx = x_test2)
eval_results(y_test2, predictions_test2, test2) #RMSE: 0.0811, Rsquare: 0.6354

```


### Model Selection


## Conclusions & Next Steps

What have I learned?

Areas for Future Research

## Bibliography

https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r


## Appendix with Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

How have we done this in the past? (ie. DATA 621)