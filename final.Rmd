---
title: 'DATA 698 Final Research Project'
author: "Magnus Skonberg"
date: "`r Sys.Date()`" # Due 5/11/2021
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: no
    theme: flatly
    highlight: tango
  pdf_document:
    latex_engine: xelatex
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

</style>
```

```{r setup, include=FALSE}
#Import libraries
library(tidyverse)
library(dplyr)
library(readr)
library(ggplot2)
library(RCurl)
library(rvest)
library(stringr)
library(tidyr)
library(kableExtra)
library(BBmisc)
library(tm)
library(sqldf)
library(inspectdf)
library(corrplot)
library(MASS)

options(scipen = 9)
set.seed(123)

#User-defined function
plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor, use = 'na.or.complete')
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}

```

## Abstract

TBD

## Introduction

### Author

### The Problem

## Literature Review

To be amended ...

## Methodology

* Data Gathering & Pre-Processing
* Data Exploration & Preparation
* Multivariate Regression Analysis


## Data Gathering & Preprocessing

### Dependent Variable Creation

In order to create our dependent 'health score' variable, I first familiarized myself with the data at hand. 

Life expectancy, obesity, and physical activity data were downloaded from the [Institute for Health Metrics and Evaluation](http://www.healthdata.org/us-health/data-download) and converted to a csv-compatible form, where then a subset of columns were selected for our consideration for the creation of a dependent 'health score' variable.

* For **life expectancy data** we read in: `Male life expectancy, 2010 (years)`, `Female life expectancy, 2010 (years)`, `Difference in male life expectancy, 1985-2010 (years)`, and	`Difference in female life expectancy, 1985-2010 (years)`.
* For **obesity data** we read in: `Male obesity prevalence, 2009 (%)`, `Female obesity  prevalence, 2009 (%)`, `Difference in male obesity prevalence, 2001-2009 (percentage points)`, and `Difference in female obesity prevalence, 2001-2009 (percentage points)`.
* For **physical activity data** we read in: `Male sufficient physical activity  prevalence, 2009 (%)`, `Female sufficient physical activity  prevalence, 2009 (%)`, `Difference in male sufficient physical activity prevalence, 2001-2009 (percentage points)`, and `Difference in female sufficient physical activity prevalence, 2001-2009 (percentage points)`.

For sake of conciseness, the majority of exploratory details and plots for our dependent variable's creation have been remitted (although the code is available in the Appendix). Prior to moving on though, it is worth noting a few points that were observed prior to the normalization and congregation of our sub-variables:

* With regard to **longevity**: on average, males live to be ~75 years old while females live to be ~80 years old. Thus, females live ~5yrs more than males on average. On average, male life expectancy increased by ~4 years while female life expectancy increased by ~1.5 years. Thus, male life expectancy increased at a greater rate than female life expectancy from 1985-2010.
* With regard to **obesity**: on average, 38% of females were obese whereas 36% of males were. Thus, females have a *slightly* higher incidence of obesity than males. On average, the male obesity rate increased by ~7.2% while the rate of female obesity increased by ~6.7%. Thus, males got fatter at a greater rate than females from 2001 to 2009.
* With regard to **physical activity**: on average, 55% of males vs. 48.7% of females received sufficient physical activity in 2009. Thus, males reported a higher level of physical activity. On average, males had a 1.9% increase in physical activity from 2001 to 2009 whereas females reported a 4.7% increase over the same period. Thus, females increased their activity levels at a greater rate than males from 2001 to 2009.

Our dependent variable is built upon these sub-variables. The above notes provide context regarding the variable upon which our regression model is to be fastened.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}

## --- Dependent Variable Creation --- ##

#Read in life expectancy data, convert to tibble, and select pertinent columns:
longevity <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/IHME_LifeExpectancy.csv")
life_table <- as_tibble(longevity)
life_table <- life_table %>% dplyr::select(1:2,13:16) %>% na.omit()

#Read in obesity data, convert to tibble, and select pertinent columns:
obesity <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/IHME_Obesity.csv")
obesity_table <- as_tibble(obesity)
obesity_table <- obesity_table %>% dplyr::select(1:2,5:6,9:10) %>% na.omit()

#Read in physical activity data, convert to tibble, and select pertinent columns:
activity <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/IHME_PhysicalActivity.csv")
act_table <- as_tibble(activity)
act_table <- act_table %>% dplyr::select(1:2,5:6,9:10) %>% na.omit()

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}
###LIFE EXPECTANCY DATA: exploration, normalization, and compilation

#Explore life expectancy data at a county level:
glimpse(life_table)
summary(life_table)

#Extract variables of interest
m1 <- life_table$`Male life expectancy, 2010 (years)`
f1 <- life_table$`Female life expectancy, 2010 (years)`
dm1 <- life_table$`Difference in male life expectancy, 1985-2010 (years)`
df1 <- life_table$`Difference in female life expectancy, 1985-2010 (years)`

#Normalize data scale to be from 0 to 1
n_m1 = (m1-min(m1))/(max(m1)-min(m1))
n_f1 = (f1-min(f1))/(max(f1)-min(f1))
n_dm1 = (dm1-min(dm1))/(max(dm1)-min(dm1))
n_df1 = (df1-min(df1))/(max(df1)-min(df1))

#Histogram of original vs. normalized data
##Life expectancy histograms
par(mfrow=c(2,2))
hist(m1, breaks=10, xlab="Age (years)", col="lightblue", main="Male life expectancy, 2010")
hist(n_m1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Male life expectancy, 2010")
hist(f1, breaks=10, xlab="Age (years)", col="lightblue", main="Female life expectancy, 2010")
hist(n_f1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Female life expectancy, 2010")

##Longevity improvement histograms
par(mfrow=c(2,2))
hist(dm1, breaks=10, xlab="Age (years)", col="lightblue", main="Male longevity improvement, 1985-2010")
hist(n_dm1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Male longevity improvement, 1985-2010")
hist(df1, breaks=10, xlab="Age (years)", col="lightblue", main="Female longevity improvement, 1985-2010")
hist(n_df1, breaks=10, xlab="Normalized Age (years)", col="lightblue", main="Female longevity improvement, 1985-2010")

#Add normalized variables together
life <- n_m1 + n_dm1 + n_f1 + n_df1

#Normalize activity to 0-1 range
n_life = (life-min(life))/(max(life)-min(life))
#head(n_life)

#Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(life, breaks=10, xlab="Score", col="lightblue", main="Longevity metric")
hist(n_life, breaks=10, xlab="Normalized Score", col="lightblue", main="Longevity metric")
summary(n_life) #slight left skew

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}
###OBESITY DATA: exploration, normalization, and compilation

#Explore obesity data at a county level:
glimpse(obesity_table)
summary(obesity_table)

#Extract variables of interest
m2 <- obesity_table$`Male obesity prevalence, 2009 (%)` 
f2 <- obesity_table$`Female obesity  prevalence, 2009 (%)` 
dm2 <- obesity_table$`Difference in male obesity prevalence, 2001-2009 (percentage points)` 
df2 <- obesity_table$`Difference in female obesity prevalence, 2001-2009 (percentage points)`

#Normalize
n_m2 = (m2-min(m2))/(max(m2)-min(m2))
n_f2 = (f2-min(f2))/(max(f2)-min(f2))
n_dm2 = (dm2-min(dm2))/(max(dm2)-min(dm2))
n_df2 = (df2-min(df2))/(max(df2)-min(df2))

#Histogram of original vs. normalized data
par(mfrow=c(2,2))
hist(m2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Male obesity prevalence, 2009")
hist(n_m2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Male obesity prevalence, 2009")
hist(f2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Female obesity prevalence, 2009")
hist(n_f2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female obesity prevalence, 2009")

par(mfrow=c(2,2))
hist(dm2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Male obesity increase, 2001-2009")
hist(n_dm2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Male obesity increase, 2001-2009")
hist(df2, breaks=10, xlab="Obesity rate (%)", col="lightblue", main="Female obesity increase, 2001-2009")
hist(n_df2, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female obesity increase, 2001-2009")

#Add normalized variables together
fat <- n_m2 + n_dm2 + n_f2 + n_df2

#Normalize activity to 0-1 range
n_fat = (fat-min(fat))/(max(fat)-min(fat))

#head(n_fat)

# Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(fat, breaks=10, xlab="Score", col="lightblue", main="Obesity metric")
hist(n_fat, breaks=10, xlab="Normalized Score", col="lightblue", main="Obesity metric")
summary(n_fat) #right skewed

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include = FALSE}
###PHYSICAL ACTIVITY DATA: exploration, normalization, and compilation

glimpse(act_table)
summary(act_table)

#Explore and normalize male physical activity data
m3 <- act_table$`Male sufficient physical activity  prevalence, 2009 (%)`
f3 <- act_table$`Female sufficient physical activity  prevalence, 2009 (%)`
dm3 <- act_table$`Difference in male sufficient physical activity prevalence, 2001-2009 (percentage points)`  
df3 <- act_table$`Difference in female sufficient physical activity prevalence, 2001-2009 (percentage points)` 

#Normalized Data
n_m3 = (m3-min(m3))/(max(m3)-min(m3))
n_f3 = (f3-min(f3))/(max(f3)-min(f3))
n_dm3 = (dm3-min(dm3))/(max(dm3)-min(dm3))
n_df3 = (df3-min(df3))/(max(df3)-min(df3))

#Histogram of original vs. normalized data
par(mfrow=c(2,2))
hist(m3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Male activity prevalence, 2009")
hist(n_m3, breaks=10, xlab="Normalized physical activity rate (%)", col="lightblue", main="Male activity prevalence, 2009")
hist(f3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Female activity prevalence, 2009")
hist(n_f3, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female activity prevalence, 2009")

par(mfrow=c(2,2))
hist(dm3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Male activity difference, 2001-2009")
hist(n_dm3, breaks=10, xlab="Normalized physical activity rate (%)", col="lightblue", main="Male activity difference, 2001-2009")
hist(df3, breaks=10, xlab="Physical activity rate (%)", col="lightblue", main="Female activity difference, 2009")
hist(n_df3, breaks=10, xlab="Normalized obesity rate (%)", col="lightblue", main="Female activity difference, 2009")

#Add normalized variables together
active <- n_m3 + n_dm3 + n_f3 + n_df3

#Normalize activity to 0-1 range
n_active = (active-min(active))/(max(active)-min(active))

#head(n_active)

# Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(active, breaks=10, xlab="Score", col="lightblue", main="Physical activity metric")
hist(n_active, breaks=10, xlab="Normalized Score", col="lightblue", main="Physical activity metric")

summary(n_active) #slight right skew

```

For each over-arching variable (life expectancy, obesity, or physical activity), we read in the 4 corresponding variables listed above, normalize, compile the over-arching variable as a summation of the normalized sub-variables and then normalize the result. 

The normalization of sub-variables and (later) over-arching variables was done in order to bring all data to a 0-to-1 scale via the following equation:

$$
Transformed.Values = \frac{(Values - Min)}{(Max - Min)}
$$

Upon normalization of our over-arching variables, we created our dependent 'healthy lifestyle' variable as a combination of longevity, obesity, and physical activity:

$$
Lifestyle = Normalized.Life - Normalized.Obesity + Normalized.Activity
$$

The result was normalized, bringing our cycle of thrice normalizing and twice congregating to a close, with a dependent variable on a 0 to 1 scale:

```{r, eval = TRUE, echo = FALSE}

###DEPENDENT VARIABLE CREATION: sum, normalize, and plot

#Add normalized variables together
lifestyle <- n_life - n_fat + n_active

#Normalize health to 0-1 range
normalized_lifestyle = (lifestyle-min(lifestyle))/(max(lifestyle)-min(lifestyle))

#Histogram of original vs. normalized data
#par(mfrow=c(1,2))
#hist(lifestyle, breaks=10, xlab="Score", col="lightblue", main="Health metric")
hist(normalized_lifestyle, breaks=10, xlab="Normalized Score", col="lightblue", main="Health metric")

summary(normalized_lifestyle)
#head(normalized_lifestyle)

```

For our 'healthy lifestyle' metric, we observe a normal distribution whose peak is centered between 0.4 and 0.5. When we consult the summary statistics, we verify a mean of 0.4639 and a median of 0.4642. Confirming that our dependent variable is slightly left skewed.

### Top 10 Healthiest Counties

As a next step, we utilize our health score metric to filter through county data for the top 10 healthiest counties:

```{r, eval = TRUE, echo = FALSE}
#create new df with state | county | health score
starter_df <- life_table %>% 
    dplyr::select(1:2)

starter_df$health_score <- normalized_lifestyle
healthiest_counties <- filter(starter_df, `health_score` > 0.895) #top 10
healthiest_counties <- healthiest_counties[order(-healthiest_counties$`health_score`),] #descending order

#head(starter_df)
#nrow(healthiest_counties) #10
healthiest_counties %>%
  kbl() %>%
  kable_minimal()

```


From the above list, we note (6) Colorado, (2) California, (1) Utah and (1) Wyoming county. From this, we extend a few *assumptions* regarding factors that might come into play for the healthiest counties: 

* sunshine, 
* median income,
* sparser population clusters (aside from San Fransisco), and
* friendliness to an active, healthy lifestyle.

We note these factors with interest and plan to revisit our assumptions later. We can observe the variables that are included in our optimal multi regression model as well as those that appear to carry the most predictive impact (ie. largest coefficients).

With our health metric created and a brief investigation into the top 10 healthiest counties, we move on to reading in, exploring, and preparing our independent variables.


### Independent Variable Preprocessing

We sought county-level data where the counties of the United States would form the basis of our observations and variables could include numerous standard and non-standard health-related metrics:

* `Alcohol Consumption` sourced from [IHME](http://ghdx.healthdata.org/record/ihme-data/united-states-alcohol-use-prevalence-county-2002-2012).
* `Heart Disease` sourced from the [IHME](http://ghdx.healthdata.org/record/ihme-data/united-states-cardiovascular-disease-mortality-rates-county-1980-2014)
* `Education`, `Unemployment`, and `Population` sourced from the [USDA](https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/).
* `Environmental Quality Index` sourced from the [EPA](https://edg.epa.gov/EPADataCommons/public/ORD/CPHEA/EQI_2006_2010/).
* `Food Insecurity` sourced **by request** from [Feeding America](https://www.feedingamerica.org/research/map-the-meal-gap/by-county).
* `Sunlight` sourced from [CDC Wonder](https://wonder.cdc.gov/controller/datarequest/D80;jsessionid=097497410B08E44FD89ECC2AB08F?stage=results&action=sort&direction=MEASURE_DESCEND&measure=D80.M1).
* `Poverty` and `MedianIncome` sourced from the [US Census Bureau](https://www.census.gov/data/datasets/2019/demo/saipe/2019-state-and-county.html).

While the IHME is an independent global health research center associated with the University of Washington and Feeding America is one of the largest nonprofit organizations in the United States, every other data source is connected to the United States government. **The reputability and dependability of the source / institution motivated the election of these sources. **

Datasets were downloaded from their respective platforms, simplified to a .csv-compatible form (*obviously* impertinent variables and header rows were dropped), uploaded to Github and then read in via R's built-in **read_csv()** function. This operation was completed (9) times because the majority of our variables were listed in separate sets.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}

## --- Independent Variable Preprocessing --- ##

#Read in alcohol data, convert to tibble, and drop impertinent observation:
alcohol <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/AlcoholConsumption.csv")
alcohol <- as_tibble(alcohol)
alcohol <- alcohol[-1,] #drop State = National
#dim(alcohol) #3178 x 6

alcohol <- alcohol[alcohol$Location != alcohol$State,] #drop state observations from Location column
#dim(alcohol) #3178 - 3127 = 51 observations dropped

#rename columns
alcohol <- alcohol %>% rename( 
    County = Location,
    Hvy = Hvy_2012,
    Bng = Binge_2012,
    HvyPctChg = `HvyPctChg_2005-2012`,
    BngPctChg = `BingePctChg_2005-2012`)

#drop excess verbage from County column
stopwords <- c("and", "Area", "Borough", "Census", "City", "County", "Division", "Municipality", "Parish")
alcohol$County <- removeWords(alcohol$County, stopwords)
#head(alcohol)

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in heart data, convert to tibble, and drop impertinent observation:
heart <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/CardiovascularDisease.csv")
heart <- as_tibble(heart)
heart <- subset(heart, select = -c(`Mortality Rate, 2010*`)) #drop 2010
heart <- heart[-1,] #drop State = National
#dim(heart) #3193 x 4

# remove State == Location
heart <- heart[heart$Location != heart$State,]
dim(heart) #3193 - 3143 = 50 observations removed

# retitle columns
heart <- heart %>% rename( 
    County = Location,
    Mortality_2005 = `Mortality Rate, 2005*`,
    Mortality_2014 = `Mortality Rate, 2014*`)

# retain value EXCLUSIVELY for Mortality Rate columns
heart$Mortality_2005 <- gsub("\\s*\\([^\\)]+\\)","",as.character(heart$Mortality_2005))
heart$Mortality_2014 <- gsub("\\s*\\([^\\)]+\\)","",as.character(heart$Mortality_2014))

#convert columns to proper type
heart$Mortality_2005 <- as.double(heart$Mortality_2005)
heart$Mortality_2014 <- as.double(heart$Mortality_2014)

#drop excess verbage from County column
heart$County <- removeWords(heart$County, stopwords)
heart$County <- gsub("(.*),.*", "\\1", heart$County) #remove everything after comma

# add Chg column
heart$MortalityChg <- heart$Mortality_2014 - heart$Mortality_2005

#finalize format of df
heart <- subset(heart, select = -c(`Mortality_2005`)) #drop 2005
heart <- heart %>% rename( Mortality = Mortality_2014)

#head(heart)

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in education data, convert to tibble, and drop impertinent observation:
education <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Education.csv")
education <- as_tibble(education)
education <- education[-1,] #drop State = National

education$State <- state.name[match(education$State, state.abb)] #convert state abbreviation to name

education <- education[education$`Area name` != education$State,] #drop state observations from Area name column
#dim(education) #3281 - 3233 = 48 observations dropped

#rename columns
education <- education %>% rename(
    County = `Area name`,
    LTHighSchool = `Percent of adults with less than a high school diploma, 2015-19`,
    HighSchool = `Percent of adults with a high school diploma only, 2015-19`,
    SomeCollege = `Percent of adults completing some college or associate's degree, 2015-19`,
    College = `Percent of adults with a bachelor's degree or higher, 2015-19`)

#drop excess verbage from County column
education$County <- removeWords(education$County, stopwords)

#head(education) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in eqi data and convert to tibble:
eqi <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/EnvironmentalQualityIndex.csv")
eqi <- as_tibble(eqi)
eqi <- subset(eqi, select = -c(3:7)) #drop indices that makeup EQI score

eqi$State <- state.name[match(eqi$State, state.abb)] #convert state abbreviation to name

#rename columns
eqi <- eqi %>% rename(
    County = County_Name,
    EQI = environmental_quality_index)

#drop excess verbage from County column
eqi$County <- removeWords(eqi$County, stopwords)

#head(eqi) #verify
#dim(eqi) #3281 x 6

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in food insecurity data and convert to tibble:
food <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/FoodInsecurity.csv")
food <- as_tibble(food)
#head(food)

#drop FIPS
food <- subset(food, select=-c(FIPS))
#dim(food) #3142 x 3: no need to drop observations

#convert State to full name
food$State <- state.name[match(food$State, state.abb)] #convert state abbreviation to name

#rename columns
food <- food %>% rename(
    County = `County, State`,
    FoodInsecurity = `2018 Food Insecurity Rate`)

#remove excess verbage from County
food$County <- removeWords(food$County, stopwords)
food$County <- gsub("(.*),.*", "\\1", food$County) #remove everything after comma

#drop % from Food Insecurity and convert to double
food$FoodInsecurity = as.double(gsub("[\\%,]", "", food$FoodInsecurity))

#head(food) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in sun data and convert to tibble:
sun <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Sunlight.csv")
sun <- as_tibble(sun)
#dim(sun) #3161 x 3

#rename column
sun <- sun %>% rename(Sun = `Avg Daily Sunlight`)

#drop excess verbage from County column
sun$County <- removeWords(sun$County, stopwords)
sun$County <- gsub("(.*),.*", "\\1", sun$County) #remove everything after comma

#head(sun) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in une ployment data and convert to tibble:
unemp <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Unemployment.csv")
unemp <- as_tibble(unemp)
unemp <- unemp[-1,] #drop State = National
unemp <- subset(unemp, select=-c(3)) #drop 2016

unemp$State <- state.name[match(unemp$State, state.abb)] #convert state abbreviation to name
unemp <- unemp[unemp$area_name != unemp$State,] #drop state observations from Area name column

unemp <- unemp %>% rename(
    County = area_name,
    Unemployment = Unemployment_rate_2019,
    UnemploymentChg = `Unemployment_chg_2016-2019`) #rename columns

unemp$County <- removeWords(unemp$County, stopwords) #drop excess verbage from County column
unemp$County <- gsub("(.*),.*", "\\1", unemp$County) #remove everything after comma

#dim(unemp) #3274 - 3224 = 50 dropped
#head(unemp) #verify

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in wealth data and convert to tibble:
wealth <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/Wealth.csv")
wealth <- as_tibble(wealth)
wealth <- wealth[-1,] #drop State = National
#dim(wealth) #3193 x 4

wealth$State <- state.name[match(wealth$State, state.abb)] #convert state abbreviation to name
wealth <- wealth[wealth$County != wealth$State,] #drop state observations from Area name column
#dim(wealth) #3193 - 3143 = 50 observations dropped

#convert columns to proper type
wealth$PovertyRate <- as.double(wealth$PovertyRate)
wealth$MedianHouseholdIncome <- as.numeric(gsub(",","",wealth$MedianHouseholdIncome))

#rename columns
wealth <- wealth %>% rename(
    Poverty = PovertyRate,
    Income = MedianHouseholdIncome) #rename columns

wealth$County <- removeWords(wealth$County, stopwords) #drop excess verbage from County column

#head(wealth)

```

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Read in population data and convert to tibble:
pop <- read_csv("https://raw.githubusercontent.com/Magnus-PS/DATA-698/data/population.csv")
pop <- as_tibble(pop)

# rename columns
pop <- pop %>% rename(
    State = STNAME,
    County = CTYNAME,
    Pop_2010 = CENSUS2010POP,
    Population = POPESTIMATE2019,
    Births = BIRTHS2019,
    Deaths = DEATHS2019,
    NetMig = NETMIG2019) #rename columns

#add population change variable
pop$PopChg <- pop$Population - pop$Pop_2010

pop <- subset(pop, select=-c(3)) #drop 2010

#dim(pop) #3193 x 7
pop <- pop[pop$County != pop$State,] #drop state observations from Area name column
#dim(pop) #3193 - 3141 = 52 observations dropped

pop$County[1802] <- "Dona Ana County" #invalid UTF-8
pop$County <- removeWords(pop$County, stopwords) #drop excess verbage from County column

#head(pop)

```

Bringing each of our sources to a consistent format was of the utmost importance for us to merge dataframes and (later) explore our data.

In order to do so:

* impertinent variables and observations were dropped,
* where `State` names were abbreviated they were converted to the full name,
* excess verbage was dropped from our `County` variable, 
* percent change variables were added (where applicable),
* excess characters were dropped from observations (ie. "%" from percent change variables),
* variables were converted to the proper type (ie. numeric variable's listed as character type),
* the general consistency of variable names, type, and format was established, and
* all dataframes were merged based on matching `State` *and* `County` observations.

```{r, comment=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Merge df's

#1. merge health score and alcohol df's
##Trim white space
alcohol$County <- trimws(alcohol$County)
starter_df$County <- trimws(starter_df$County)
##SQL join
df <- sqldf("SELECT *
             FROM starter_df
             LEFT JOIN alcohol ON starter_df.State = alcohol.State AND starter_df.County = alcohol.County")
##remove extra State, County columns
df <- subset(df, select=-c(4,5))

#2. merge heart to df
##Trim white space
heart$County <- trimws(heart$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN heart ON df.State = heart.State AND df.County = heart.County")
##remove extra State, County columns
df <- subset(df, select=-c(8,9))

#3. merge education to df
##Trim white space
education$County <- trimws(education$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN education ON df.State = education.State AND df.County = education.County")
##remove extra State, County columns
df <- subset(df, select=-c(10,11))

#4. merge eqi to df
##Trim white space
eqi$County <- trimws(eqi$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN eqi ON df.State = eqi.State AND df.County = eqi.County")
##remove extra State, County columns
df <- subset(df, select=-c(14,15))

#5. merge food to df
##Trim white space
food$County <- trimws(food$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN food ON df.State = food.State AND df.County = food.County")
##remove extra State, County columns
df <- subset(df, select=-c(15,16))

#6. merge sun to df
##Trim white space
sun$County <- trimws(sun$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN sun ON df.State = sun.State AND df.County = sun.County")
##remove extra State, County columns
df <- subset(df, select=-c(16,17))

#7. merge unemp to df
##Trim white space
unemp$County <- trimws(unemp$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN unemp ON df.State = unemp.State AND df.County = unemp.County")
##remove extra State, County columns
df <- subset(df, select=-c(17,18))

#8. merge wealth to df
##Trim white space
wealth$County <- trimws(wealth$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN wealth ON df.State = wealth.State AND df.County = wealth.County")
##remove extra State, County columns
df <- subset(df, select=-c(19,20))

#9. merge pop to df
##Trim white space
pop$County <- trimws(pop$County)
##SQL join
df <- sqldf("SELECT *
             FROM df
             LEFT JOIN pop ON df.State = pop.State AND df.County = pop.County")
##remove extra State, County columns
df <- subset(df, select=-c(21,22))

#verify variables and dimensions
head(df) 
dim(df) #3154 x 25

```

The result of our merging all dataframes / variables into one master is a 3154 observation x 25 variable dataframe `df`. We'll explore the resulting data frame in the next section to see what insight we might glean to better inform our data preparation and model-building.


## Data Exploration & Preparation

The goal of exploratory data analysis (or EDA for short) is to really grasp and understand the data at hand.

For our approach, we first get to know the structure and value ranges, we then look at the distributions of our features, visualize the relationship our variables have with one another and the target variable via correlation matrix, and visit the proportion of missing data vs. correlation with target. After this point, we should have enough insight to prepare our data and then build our model.

To start, we utilize the built-in glimpse() and summary() methods to gain insight into the dimensions, variable characteristics, and value range for our training dataset:

```{r, echo = F, eval = T}

#baseline EDA: glimpse() and summary()
glimpse(df)
#summary(df)

```

For sake of concise, the summary() results have been omitted but from the glimpse() results we observe that we're dealing with 2 categorical variables, 23 numeric variables, significant NA counts for numerous variables, and quite a difference in the magnitude of values based on the variables.

Our **categorical variables** are `State`, our state identifier and `County`, our county identifier. These variables are not of much use for anything beyond identification and may be excluded from EDA and model-building.

Our **dependent variable** `health_score` is quantitative and provides a measure of 'healthy lifestyle' as a function of longevity, obesity, and physical activity. This will be our "target" variable, the one against which we'll measure the impact of our independent variables.

As for our **quantitative, independent variables**:

* `Hvy`: % of population that drink heavily. *"Heavy" drinking is defined as the consumption, on average, of more than one drink per day for women or two drinks per day for men in the past 30 days.*
* `HvyPctChg`: % change in heavy drinking from 2005 to 2009.
* `Bng`: % of population that binge drink. *"Binge" drinking is defined as the consumption of more than four drinks for women or five drinks for men on a single occasion at least once in the past 30 days.*
* `BngPctChg`: % change in binge drinking from 2005 to 2009.
* `Mortality`: mortality rate in 2014 as a result of heart disease.
* `MortalityChg`: % change in mortality rate from 2005 to 2014 as a result of heart disease.
* `LTHighSchool`: % of population educated less than high school from 2015-2019.
* `HighSchool`: % of population educated at a high school level (and no further) from 2015-2019.
* `SomeCollege`: % of population who received some college education from 2015-2019.
* `College`: % of population who completed at least a Bachelor's level education from 2015-2019.
* `EQI`: Environmental Quality Index.
* `FoodInsecurity`: % of population whose food intake was disrupted by lack of resources in 2018.
* `Sun`: average daily sunlight (KJ/m^2) in 2011.
* `Unemployment`: rate of unemployment in 2019.
* `UnemploymentChg`: change in unemployment rate from 2015 to 2019.
* `Poverty`: rate of poverty in 2019.
* `Income`: median household income in 2019.
* `Population`: county-level population for 2019.
* `Births`: county-level births for 2019.
* `Deaths`: county-level deaths for 2019.
* `NetMig`: county-level net migration for 2019.
* `PopChg`: county-level rate of population change from 2010 to 2019.

As a next step, we drop NA values from consideration (150 observations) and turn our attention to exploring the relationship these variables have with one another and with the target via **correlation matrix**. We consider only variables with a correlation significance > 0.2 in our plot: 

```{r, eval = F, echo = F}

#drop NAs from consideration
df <- drop_na(df)
dim(df) #3154 - 3004 = 150 dropped observations

```

```{r, echo = F, eval = T, fig.height = 8, fig.width = 10}

#Select numeric variables
df_num <- as.data.frame(df[3:25])
#dim(df_num)
#head(df_num)

#Utilize custom-built correlation matrix generation function
plot_corr_matrix(df_num, 0.2)

```

The above plot is "busy", and from that, we may extend that **multicollinearity is a MAJOR concern**. We may eliminate a large portion of the variables we've carried upto this point and proceed with *only* those with strong predictive promise and / or unique value added (ie. `BngPctChg` or `Sun`).

Based on these guidelines, it appears that we should proceed with *at least* `BngPctChg`, `Mortality`, `College`, `FoodInsecurity`, `Unemployment`, `Income`, and `PopChg`. These variables are relatively unique from one another and have a strong correlation with `health_score`.

To clarify we consult a table with the proportion of missing data and correlation with our target variable:

```{r, echo = F, eval = T}
#Compute proportion of missing data per variable
v <- colnames(df_num)
incomplete <- function(x) sum(!complete.cases(x)) / 3004
Missing_Data <- sapply(df_num[v], incomplete) 
#head(Missing_Data) #verify

#Compute correlation between each variable and TARGET
target_corr <- function(x, y) cor(y, x, use = "na.or.complete")
HealthScore_Corr <- sapply(df_num[v], target_corr, y=df_num$health_score) 
#head(HealthScore_Corr) #verify

#Bind and output Missing Data and Correlation with Target
MDHSC <- data.frame(cbind(Missing_Data, HealthScore_Corr))
MDHSC %>%
  kbl(caption = "Proportion of Missing Data vs. Correlation with Health Score") %>%
  kable_minimal()

```

From the Missing_Data column we see that all of our independent variables are missing 2-3% of their data. This may be solved via imputation (KNN or median value).

From the HealthScore_Corr column we see that `Hvy`, `MortalityChg`, `NetMig` have a **weak** correlation with `health_score`. Whereas the remainder of our variables carry a relatively strong (positive or negative) correlation with the target variable and would only be excluded due to multicollinearity.

To address weak target correlation and multicollinearity we consider the exclusion of 13 variables from further consideration:

* `Hvy`, `MortalityChg`, and `NetMig` due to **weak correlation** with the target variable
* `HvyPctChg` `Bng`, `LTHighSchool`, `HighSchool`, `SomeCollege`, `Unemployment`, `Poverty`, `Population`, `Births`, and `Deaths` due to **multicollinearity**.

Before doing so, we utilize the **Boruta** function for feature ranking and selection as confirmation (included in Appendix). In doing so, `MortalityChg`, `BngPctChg`, `Unemployment`, `HvyPctChg`, and `Population` were identified as the variables with the lowest importance. 

```{r, echo = F, eval = F, fig.height = 8, fig.width = 10}
#Utilize Boruta for feature ranking and selection
library(Boruta)

# Perform Boruta search
boruta_output <- Boruta(health_score ~ ., data=na.omit(df_num), doTrace=0, maxRuns = 1000)

#Get significant variables including tentatives
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
#print(boruta_signif)

# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")

```

We observe that `Mortality`, `College`, `FoodInsecurity`, `LTHighSchool`, `Sun`, `EQI`, `Income`, `PopChg`, `Bng`, and `UnemploymentChg` appear to be our strongest predictors (with the lowest incidence of multicollinearity).

Based on variable importance, correlation with `health_score` and multicollinearity, we'll proceed through data preparation with just these variables and visit their distributions before proceeding further with our data preparation:

```{r, eval = F, echo = F}
#Feature selection (address weak target correlation and multicollinearity)
select_vars <- c('health_score','Mortality','College','FoodInsecurity','LTHighSchool','Sun','EQI','Income','PopChg','Bng','UnemploymentChg')
select_df <- df_num[select_vars]
#head(select_df) #verify

```

```{r, eval = TRUE, echo = FALSE, fig.height = 8, fig.width = 10}
#Histograms for all variables
select_df %>%
    gather() %>%
    ggplot(aes(value)) +
        facet_wrap(~ key, scales = "free", ncol=5) +
        geom_histogram(bins=20,color="darkblue", fill="lightblue")

```

From the histograms above, we note a number of **non-normal** (ie. `PopChg`), **bimodal** (ie. `Sun`), **normal right skewed** (ie. `College`), **normal left skewed** (ie. `EQI`), and relatively **normal** (ie. `Mortality`) distributions. 

The wide-ranging difference in scales and non-normal distributions may be problematic in applying generalized linear regression models. Normalization may address the scales issue while transformation, outlier removal, or feature creation (ie. converting non-normal variables to dummy "flag" variables) may address the issue of non-normal distributions.

```{r}
#Imputation / removal (address NAs)

#Normalization (address difference in scales)

#Outliers (address observations that hinder predictive prowess)

#Feature creation?

```

## Multivariate Regression Analysis

```{r}

#Baseline model (model 1)
# df_base <- subset(df, select=-c(1,2)) #categorical columns
# model_1 <- lm(health_score ~., data = df_base)
# summary(model_1) #R^2 = 0.6725, vars = 23
# 
# #AIC optimized model (model 2)
# stepAIC(model_1)
# model_2 <- lm(formula = health_score ~ Hvy + HvyPctChg + Bng + BngPctChg + 
#     Mortality + HighSchool + SomeCollege + College + EQI + FoodInsecurity + 
#     Sun + Unemployment + UnemploymentChg + Poverty + Population + 
#     Births + Deaths + NetMig, data = df_base)
# summary(model_2) #R^2 = 0.672, vars = 18

```

## Conclusions & Next Steps

What have I learned?

Areas for Future Research

## Bibliography

## Appendix with Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

How have we done this in the past? (ie. DATA 621)